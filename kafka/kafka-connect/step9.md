That's all !

## Summary

In this scenario, we :

* Use the new `confluent` products :
  * [schema-registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
  * [`kafka-connect`](https://docs.confluent.io/platform/current/connect/index.html)
* Consume topic through kafka-connect and copy them in an output database.
* Simulate an unreachable database issue and the recovery data behaviour
* Execute a complete data's management scenario.

## Useful links and Resources

* [Github](https://github.com/adriens/presentation-kafka-connect) repository
* [Slides presentation](https://adriens.github.io/presentation-kafka-connect/)
* Do you know [Bloat](https://www.katacoda.com/glialabs/courses/postgresql/purge-partitions) ?
* What is [Kafka connect](https://docs.confluent.io/platform/current/connect/index.html) ?
* [Schema Management](https://docs.confluent.io/platform/current/schema-registry/index.html) Overview
